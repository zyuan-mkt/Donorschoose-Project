{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN Wordcloud.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEZL3qwHsAdp",
        "outputId": "c9788a6a-048c-4973-8203-8e1e7ba2cc16"
      },
      "source": [
        "!pip install captum\n",
        "import pandas as pd\n",
        "import numpy  as np\n",
        "import spacy\n",
        "sp = spacy.load('en')\n",
        "all_stopwords = sp.Defaults.stop_words\n",
        "import nltk \n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "words = set(nltk.corpus.words.words())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting captum\n",
            "  Downloading captum-0.4.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 30.1 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |▊                               | 30 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 92 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 102 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 112 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 122 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 133 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 143 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 153 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 163 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████                            | 174 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 184 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 194 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 204 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████                           | 215 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 225 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 235 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 245 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 256 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 266 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 276 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 286 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 296 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████                         | 307 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 317 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 327 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 337 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 348 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 358 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 368 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 378 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 389 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 399 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 409 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 419 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 430 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 440 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 450 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 460 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 471 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 481 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 491 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 501 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 512 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 522 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 532 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 542 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 552 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 563 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 573 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 583 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 593 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 604 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 614 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 624 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 634 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 645 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 655 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 665 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 675 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 686 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 696 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 706 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 716 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 727 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 737 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 747 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 757 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 768 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 778 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 788 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 798 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 808 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 819 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 829 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 839 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 849 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 860 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 870 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 880 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 890 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 901 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 911 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 921 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 931 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 942 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 952 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 962 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 972 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 983 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 993 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.0 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.0 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.0 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.0 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.1 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.1 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.1 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.1 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.1 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.1 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.1 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.1 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.2 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.2 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.2 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.2 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.2 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.2 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.2 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.2 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.2 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.3 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.3 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.3 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.3 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.3 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.3 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.3 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.3 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.4 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.4 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.4 MB 7.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4 MB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2 in /usr/local/lib/python3.7/dist-packages (from captum) (1.10.0+cu111)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from captum) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from captum) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2->captum) (3.10.0.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->captum) (1.15.0)\n",
            "Installing collected packages: captum\n",
            "Successfully installed captum-0.4.1\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGkkb6xmsDGv"
      },
      "source": [
        "import captum\n",
        "from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization\n",
        "\n",
        "import time \n",
        "import random\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import Vocab\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext.legacy.data import Field,LabelField, Dataset,TabularDataset, BucketIterator, Iterator"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NlDH0O-sDJD"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
        "                 dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.conv_0 = nn.Conv2d(in_channels = 1, \n",
        "                                out_channels = n_filters, \n",
        "                                kernel_size = (filter_sizes[0], embedding_dim))\n",
        "        \n",
        "        self.conv_1 = nn.Conv2d(in_channels = 1, \n",
        "                                out_channels = n_filters, \n",
        "                                kernel_size = (filter_sizes[1], embedding_dim))\n",
        "        \n",
        "        self.conv_2 = nn.Conv2d(in_channels = 1, \n",
        "                                out_channels = n_filters, \n",
        "                                kernel_size = (filter_sizes[2], embedding_dim))\n",
        "        \n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):\n",
        "                \n",
        "        #text = [batch size, sent len]\n",
        "        \n",
        "        embedded = self.embedding(text)\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        \n",
        "        #embedded = [batch size, 1, sent len, emb dim]\n",
        "        \n",
        "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
        "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
        "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
        "            \n",
        "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "        \n",
        "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
        "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
        "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
        "        \n",
        "        #pooled_n = [batch size, n_filters]\n",
        "        \n",
        "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim = 1))\n",
        "\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "            \n",
        "        return self.fc(cat)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naTG7tfysDLO"
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "    \n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    import time \n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFQ7n4tOsDNb"
      },
      "source": [
        "def train_test(model,train_iterator,valid_iterator,test_iterator,optimizer,criterion):\n",
        "    N_EPOCHS = 10\n",
        "    best_valid_loss = float('inf')\n",
        "\n",
        "    for epoch in range(N_EPOCHS):\n",
        "        start_time = time.time()\n",
        "            \n",
        "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "        \n",
        "        end_time = time.time()\n",
        "\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        \n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/education project/models/tut4-model.pt')\n",
        "        \n",
        "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "    model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/education project/models/tut4-model.pt'))\n",
        "\n",
        "    test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "    print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_KmufQBsDPt"
      },
      "source": [
        "def predict_sentiment(model, sentence, min_len = 5):\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in sp.tokenizer(sentence)]\n",
        "    if len(tokenized) < min_len:\n",
        "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    prediction = torch.sigmoid(model(tensor))\n",
        "    return prediction.item()\n",
        "def forward_with_sigmoid(input):\n",
        "    return torch.sigmoid(model(input))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sWf4a5psDSA"
      },
      "source": [
        "def implement_cnn(path,name,device):\n",
        "    SEED = 1234\n",
        "    random.seed(SEED)\n",
        "    np.random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    TEXT = Field(tokenize = 'spacy', \n",
        "                      tokenizer_language = 'en_core_web_sm',\n",
        "                      batch_first = True)\n",
        "    LABEL = LabelField(dtype = torch.float)\n",
        "\n",
        "    \n",
        "    train, validation, test = TabularDataset.splits(fields=[('text', TEXT), ('label', LABEL)],\n",
        "                                          train='train_'+ name+'.csv',\n",
        "                                          validation='val_'+name +'.csv',\n",
        "                                          test='test_'+name +'.csv',\n",
        "                                          format='CSV',\n",
        "                                          path=path)\n",
        "\n",
        "    MAX_VOCAB_SIZE = 25000\n",
        "    \n",
        "    \n",
        "    TEXT.build_vocab(train, \n",
        "                    max_size = MAX_VOCAB_SIZE, \n",
        "                    vectors = \"glove.6B.100d\", \n",
        "                    unk_init = torch.Tensor.normal_)\n",
        "    LABEL.build_vocab(train)\n",
        "    LABEL.vocab.itos = LABEL.vocab.itos[:2]\n",
        "    # LABEL.vocab.itos[0] = \"0.0\"\n",
        "    # LABEL.vocab.itos[1] = \"1.0\"\n",
        "    print(\"The reference label is {}\".format(LABEL.vocab.itos[1]))\n",
        "    print(LABEL.vocab.itos)\n",
        "\n",
        "  \n",
        "\n",
        "    BATCH_SIZE = 64\n",
        "    train_iterator = BucketIterator(train, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.text),\n",
        "                                device=device, train=True, sort=True, sort_within_batch=True)\n",
        "    valid_iterator = BucketIterator(validation, batch_size=BATCH_SIZE, sort_key=lambda x: len(x.text),\n",
        "                                device=device, train=True, sort=True, sort_within_batch=True)\n",
        "    test_iterator = Iterator(test, batch_size=BATCH_SIZE, device=device, train=False, shuffle=False, sort=False)\n",
        "\n",
        "    INPUT_DIM = len(TEXT.vocab)\n",
        "    EMBEDDING_DIM = 100\n",
        "    N_FILTERS = 100\n",
        "    FILTER_SIZES = [3,4,5]\n",
        "    OUTPUT_DIM = 1\n",
        "    DROPOUT = 0.5\n",
        "    PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] ## if length < max_len\n",
        "\n",
        "    model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
        "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "    pretrained_embeddings = TEXT.vocab.vectors\n",
        "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "    UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    model = model.to(device)\n",
        "    criterion = criterion.to(device)\n",
        "    train_test(model,train_iterator,valid_iterator,test_iterator,optimizer,criterion)\n",
        "\n",
        "    return model,TEXT,LABEL"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2dhdcgUs28-"
      },
      "source": [
        "# accumalate couple samples in this array for visualization purposes\n",
        "def interpret_sentence(model, sentence, min_len = 500, label = 0):\n",
        "    text = [tok.text for tok in sp.tokenizer(sentence.lower())]\n",
        "    if len(text) < min_len:\n",
        "        text += [TEXT.pad_token] * (min_len - len(text))\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in text]\n",
        "\n",
        "    model.zero_grad()\n",
        " \n",
        "    input_indices = torch.tensor(indexed, device=device)\n",
        "    input_indices = input_indices.unsqueeze(0)\n",
        "    \n",
        "    # input_indices dim: [sequence_length]\n",
        "    seq_length = min_len\n",
        "\n",
        "    # predict\n",
        "    pred = forward_with_sigmoid(input_indices).item()\n",
        "    pred_ind = round(pred)\n",
        "\n",
        "    # generate reference indices for each sample\n",
        "    reference_indices = token_reference.generate_reference(seq_length, device=device).unsqueeze(0)\n",
        "    # compute attributions and approximation delta using layer integrated gradients\n",
        "    attributions_ig, delta = lig.attribute(input_indices, reference_indices, \\\n",
        "                                           n_steps=min_len, return_convergence_delta=True)\n",
        "\n",
        "    # print('pred: ', Label.vocab.itos[pred_ind], '(', '%.2f'%pred, ')', ', delta: ', abs(delta))\n",
        "\n",
        "    add_attributions_to_visualizer(attributions_ig, text, pred, pred_ind, label, delta, vis_data_records_ig)\n",
        "    return(pred_ind,label)\n",
        "    \n",
        "def add_attributions_to_visualizer(attributions, text, pred, pred_ind, label, delta, vis_data_records):\n",
        "    attributions = attributions.sum(dim=2).squeeze(0)\n",
        "    attributions = attributions / torch.norm(attributions)\n",
        "    attributions = attributions.cpu().detach().numpy()\n",
        "\n",
        "    # storing couple samples in an array for visualization purposes\n",
        "\n",
        "    vis_data_records.append(visualization.VisualizationDataRecord(\n",
        "                            attributions,\n",
        "                            pred,\n",
        "                            LABEL.vocab.itos[pred_ind],\n",
        "                            label,\n",
        "                            LABEL.vocab.itos[1],\n",
        "                            attributions.sum(),       \n",
        "                            text,\n",
        "                            delta))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8UzihSfs8n3"
      },
      "source": [
        "def main_cnn(path,name,device):\n",
        "    model,TEXT,LABEL = implement_cnn(path,name,device)\n",
        "    print('Vocabulary Size: ', len(TEXT.vocab))\n",
        "    PAD_IND = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "    token_reference = TokenReferenceBase(reference_token_idx=PAD_IND)\n",
        "\n",
        "    lig = LayerIntegratedGradients(model, model.embedding)\n",
        "  \n",
        "    test_data = pd.read_csv(path+name+\".csv\")\n",
        "    # vis_data_records_ig = []\n",
        "    embeding_dic = {}\n",
        "    for index,row in test_data.iterrows():\n",
        "        Index_test = []\n",
        "        sentence = test_data.loc[index,\"text\"]\n",
        "        text = [tok.text for tok in sp.tokenizer(sentence.lower())]\n",
        "        if len(text) < 500:\n",
        "            text += [TEXT.pad_token] * (500 - len(text))\n",
        "        indexed = [TEXT.vocab.stoi[t] for t in text]\n",
        "        model.zero_grad()\n",
        "        input_indices = torch.tensor(indexed, device=device)\n",
        "        input_indices = input_indices.unsqueeze(0)\n",
        "        reference_indices = token_reference.generate_reference(len(indexed), device=device).unsqueeze(0)\n",
        "        attributions_ig, delta = lig.attribute(input_indices, reference_indices, \\\n",
        "                                                  n_steps=500, return_convergence_delta=True)\n",
        "        attributions = attributions_ig.sum(dim=2).squeeze(0)\n",
        "        attributions = attributions / torch.norm(attributions)\n",
        "        attributions = attributions.cpu().detach().numpy()\n",
        "        for i in range(500):\n",
        "            try:\n",
        "                embeding_dic[text[i]].append(attributions[i])\n",
        "            except KeyError:\n",
        "                embeding_dic[text[i]] = [attributions[i]]\n",
        "\n",
        "    avgDict = {}\n",
        "    for k,v in embeding_dic.items():\n",
        "        avgDict[k] = sum(v)/ float(len(v))\n",
        "    words_imp = [(k, v) for k, v in avgDict.items()]\n",
        "    words_imp.sort(key=lambda x: x[1])\n",
        "\n",
        "    # return vis_data_records_ig,words_imp\n",
        "    return words_imp"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBXUCzPOtEQU"
      },
      "source": [
        "def gen_wc(words_imp,output_path,name,title1,title,reverse=False):\n",
        "    words = pd.DataFrame(words_imp,columns=[\"word\",\"score\"])\n",
        "    res = []\n",
        "    tw_pairs={}\n",
        "    for index, row in words[:50].iterrows():\n",
        "        tw_pairs[row[0]] = abs(float(row[1]))\n",
        "    res.append(tw_pairs)\n",
        "\n",
        "    tw_pairs={}\n",
        "    for index, row in words[-50:].iterrows():\n",
        "        tw_pairs[row[0]] = float(row[1])\n",
        "    res.append(tw_pairs)\n",
        "    plot_wc(res,name,title1,title2,reverse)\n",
        "\n",
        "    return\n",
        "\n",
        "def plot_wc(res,name,title1,title2,reverse=False):\n",
        "    plt.figure(figsize=(20,10))\n",
        "    \n",
        "    for d in range(len(res)):\n",
        "        wc = WordCloud(width= 300, height = 300, background_color=\"white\", repeat=False)\n",
        "        wc.generate_from_frequencies(res[d])\n",
        "        \n",
        "        if reverse:\n",
        "            plt.subplot(1,2,len(res)-d)\n",
        "            plt.title(title2[len(res)-1-d],size=18,weight='bold', pad=20)\n",
        "        else:\n",
        "            plt.subplot(1,2,d+1)\n",
        "            plt.title(title2[d],size=18,weight='bold', pad=20)\n",
        "        plt.axis(\"off\")\n",
        "        plt.imshow(wc, interpolation=\"bilinear\")\n",
        "    plt.suptitle('Word Clouds Based on CNN Model of {}'.format(title1),size=20,weight='bold')\n",
        "    plt.savefig(output_path+name+'.png', facecolor=\"w\",format='png')\n",
        "    \n",
        "    return"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujyU-zV9tT-A"
      },
      "source": [
        "### Implement\n",
        "path = ### the directory you store your files\n",
        "name= ### csv file name\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "words_imp = main_cnn(path,name,device)\n",
        "name = ### png file name \n",
        "title1 = ### main title\n",
        "title2= ### subtitles\n",
        "gen_wc(words_imp,name,title1,title2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}